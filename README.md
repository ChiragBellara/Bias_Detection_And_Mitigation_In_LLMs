# Detection and Quantifying bias in Large Language Models.

## Overview:
This repository contains implementations for detecting and mitigating bias in the BOLD and CrowSpair datasets. Bias in datasets can lead to unfair or skewed outcomes in machine learning models trained on them. By detecting and mitigating bias, we aim to create more equitable and reliable models.
